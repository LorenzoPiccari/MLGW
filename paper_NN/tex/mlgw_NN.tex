\documentclass[twocolumn,showpacs,preprintnumbers,nofootinbib,prd,
superscriptaddress,10pt]{revtex4-1}

\usepackage{amsmath,amssymb}
\usepackage{amsfonts}
\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{afterpage}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{mathtools}
\usepackage{tensor}
\usepackage{layouts}
\usepackage{DejaVuSans}
\usepackage{epstopdf}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{siunitx}
	\sisetup{output-decimal-marker={.}}
	
	%some math symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\DeclareMathOperator{\sign}{sign}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}
%argmin and argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% comments command
\newcommand{\stefano}[1]{{\textcolor{red}{\texttt{S: #1}} }}
\newcommand{\tim}[1]{{\textcolor{green}{\texttt{T: #1}} }}
\newcommand{\chinmay}[1]{{\textcolor{blue}{\texttt{C: #1}} }}
\newcommand{\oldnewtxt}[2]{\sout{#1}\textcolor{red}{#2}}

\begin{document}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT
\begin{abstract}

WRITEME

\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE
	\title{mlgw NN}
	\author{Tim \surname{Grimbergen}}
        \affiliation{Institute for Gravitational and Subatomic Physics (GRASP),
		Utrecht University, Princetonplein 1, 3584 CC Utrecht, The Netherlands}
	%
	\author{Stefano \surname{Schmidt}}
		\email{s.schmidt@uu.nl}
        \affiliation{Institute for Gravitational and Subatomic Physics (GRASP),
		Utrecht University, Princetonplein 1, 3584 CC Utrecht, The Netherlands}
        \affiliation{Nikhef, Science Park 105, 1098 XG, Amsterdam, The Netherlands}
	%
	\author{Chinmay \surname{Kalaghatgi}}
        \affiliation{Institute for Gravitational and Subatomic Physics (GRASP),
		Utrecht University, Princetonplein 1, 3584 CC Utrecht, The Netherlands}
        \affiliation{Nikhef, Science Park 105, 1098 XG, Amsterdam, The Netherlands}
	%
	\author{Chris \surname{van den Broeck}}
        \affiliation{Institute for Gravitational and Subatomic Physics (GRASP),
		Utrecht University, Princetonplein 1, 3584 CC Utrecht, The Netherlands}
        \affiliation{Nikhef, Science Park 105, 1098 XG, Amsterdam, The Netherlands}
	\maketitle

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY  
\section{Introduction}
\label{sec:intro}

With almost a hundred of confirmed detections, Gravitational Waves (GW) astronomy is entering a mature state, where many loud GW events will force the scientific community to develop a fast analysis to deliver precision measurement. The recent transient catalogue GWTC-3 \cite{} is the latest achievment of the effort carried on by the LIGO-Virgo-KAGRA collaboration and it relies on both instrument and data analysis development.

A crucial element of the data analysis is the ability to generate accurate prediction of the GW signal emitted by a Binary Black Hole (BBH) system. Such waveforms are used for the expensive bayesian estimation of the parameters characterizing a BBH: the analysis of a single event requires the online generation of up to a billions of waveform.
As we move towards the next generation of detectors, it is mandatory to deploy accurate waveform models, that are fast and, at the same time, incorporates many phyics effect. Failing to do this may lead to systematic errors in the parameter recovery, due to a bad modelling of the source.
This is challenging since, speed and accuracy are often at trade.

One importante piece for a realistic BBH signal is the inclusion of the Higher Order Modes (HMs) of the multipole expansion of the waveform. For mass symmetric systems, the leading mode is orders of magnitude larger and including HMs doesn't affect the parameter estimation. On the other hand, it has been shown \cite{} that HMs are obervable in very asymmetric binary systems, hence the need to include them to avoid any bias.

Throughout decades of developments, two families of models have been developed, both being able to incorporate HMs.
One family relies on the Effective One Body (EOB) formalism, which maps the complicated general relativistic binary system into the problem governed by an effective Hamiltonian. EOB models tend to be accurate but are quite costly to evaluate, since for each waveform one needs to solve the hamiltonian equation of motion.
On the other hand, the phenomenological waveforms are based on analytical expressions (within the post-newtonian formalism). They tend to be faster to evaluate, but don't achieve the same accuracy.
Both families, EOB and phenomenological, need to be calibrated with Numerical Relativity waveform, computed by solving directly the Einstein equations. The calibration makes sure that a model retains its accuracy even close to merger, where an approximate treatment, such as the post-newtonian or EOB formalism are not applicable anymore.

Besides the standard families, surrogate waveform models have been developed for years with the intent of reproducing the output of a target model and of making feasible the usage of the underlying model. Several surrogates have been developed to accelarate several EOB models \cite{}, even including HMs. While traditional surrogate models \cite{} builds an empirical interpolant on the waveform space, a more modern approach relies on performing a regression using Machine Learning techniques \cite{}.

Among others, \cite{Schmidt:2020yuu} introduced a Machine Learning surrogate model, based on a dimensionality reduction scheme followed by a regression. In this work, we extend such model to HMs and we improve the accuracy of the regression. Our model marks a step towards the development of a fast, yet precise, waveform model, by achieving state-of-the-art accuracy and speed, and will enable the accurate analysis of the next generation detectors' data.

We train our model on the widely used approximant \texttt{IMRPhenomTHM} \stefano{probably we should really make it on an EOB model. The speed up will be much large, making a stronger case for our paper.} and we achieve a ??\% faithful when averaged across a wide range in the parameter space.
Our experiments showed that our model offers a substantial speed up with respect to the original model, matching the speed of the state-of-the-art surrogate models.

This paper is organized as follows. In Sec.~\ref{sec:model} we introduce the details of the model presented here, stressing the differences with the model in  \cite{Schmidt:2020yuu}.
Sec.~\ref{sec:performance} is devoted to the validation of our model: we will motivate our choice of several hyperparameters and perform an accuracy a speed study.
In Sec.~\ref{sec:end}, we present some final remarks and highlight future perspective.

\begin{itemize}
	\item Usual crap about GW astronomy + need for speed and accuracy
	\item BBH waveform models: predict GR but they are not fast enough
	\item Surrogates
	\item ML surrogate
\end{itemize}


 \cite{Schmidt:2020yuu}

%%%%%%%%%%%%%%%%%%%
\section{Building the model}
\label{sec:model}

A non-precessing BBH can be described by four {\it intrisic} parameters, which specify the two BH masses $m_1$ and $m_2$ and the z-component of the two spin $s_1$ and $s_2$.
Moreover, since the total mass $M = m_1 + m_2$ sets an overall amplitude scaling, the non-precessing BBH signal only depends on the mass ratio $q = m_1/m2 \geq 1$ as well as on the spins. We may refers to these parameter as $\boldsymbol{\vartheta} = (q, s_1, s_2)$.
Besides the masses and spins, the gravitational wave emitted by the system depends also on luminosity distance to the source $d_L$, the inclination angle $\iota$ of the source and the reference phase $\varphi_0$: we call them {\it extrinsic}.

As it is standard, we expand the angular dependence on $\iota, \varphi_0$ of the {\it complex} waveform $h(t)$ in terms of a sum of spin -2 spherical harmonics, leading to the following expression for the waveform:
A GW is then parametrized\footnote{Such parametrization is particularly convenient as it separates the waveform dependence over intrinsic and extrisinc parameters.} as~\cite{Estelles:2021gvs}:
\begin{align} \label{eq:h_parametrization}
	&h(t; d_L,\iota,\varphi_0, m_1, m_2, s_1, s_2) = h_+ + i h_\times \nonumber \\
		&\qquad= \frac{G}{c^2} \frac{M}{d_L}\sum_{\ell = 2}^{\infty} \sum_{m = -\ell}^{\ell} \tensor[^{-2}]{Y}{_{\ell m}}(\iota, \varphi_0) h_{\ell m}(t/M; \boldsymbol{\vartheta})
\end{align}
where we refer to the functions $h_{\ell m}(t; \boldsymbol{\vartheta})$ as {\it modes} of the waveform. We note that that, for non-precessing systems, $h_{\ell m} = (-1)^\ell h^*_{\ell -m}$.

In this work we introduce a Machine Learning model to perform a regression
\begin{align}\label{eq:objective}
	(q, s_1, s_2) &\longmapsto h_{\ell m}(t; \boldsymbol{\vartheta})
\end{align}
for each mode $(\ell,m)$.
The regression is designed to reproduce waveforms from a given dataset; such waveforms can be generated by {\it any} time-domain approximant.

We decompose each mode in amplitude and phase
\begin{equation}
	h_{\ell m}(t; \boldsymbol{\vartheta}) = A_{\ell m}(t; \boldsymbol{\vartheta}) e^{i \phi_{\ell m}(t; \boldsymbol{\vartheta})}
\end{equation}
and, for each mode, we perform a regression for amplitude and phase separately. The regression scheme closely follows \cite{Schmidt:2020yuu} and relies on:
\begin{enumerate}[label=(\alph*)]
	\item a suitable vector representation of the regression target by choosing a fixed time grid
	\item a Principal Component Analysis (PCA) model to reduce the dimensionality of each waveform
	\item an Artificial Neural Network (ANN) regression to learn the dependence on $\boldsymbol{\vartheta}$ of the reduced waveform
\end{enumerate}

While the first two elements are unchanged from the previous work, the NN regression is first introduced here. Indeed a NN has more representation power than the Mixture of Experts (MoE) model \cite{Jacobs1991AdaptiveMoE}, used in \cite{Schmidt:2020yuu}: the change was needed to achieve better accuracy for the model.

\subsection{Dataset creation}
\label{sec:dataset}

To construct a dataset, we follow \cite{Schmidt:2020yuu} and we set a dimensionless time grid. We construct the grid by setting $D$ points equally spaced in $\tau^\alpha$, where $\tau = t/M$. Using the findings of \cite{Schmidt:2020yuu}, we set $D = \text{???}$ and $\alpha = \text{???}$.
%
This is a good compromise between the need of having a faithful representation of the waveform (which requires a large grid) and the need of having a compact model (which points to a sparse grid).

The starting point of the grid $\tau_0$ sets the length of the waveform that our model is able to generate. We choose $\tau_0 = \text{???}$.

Once a time grid is set, we evaluate all the modes (amplitude and phase) on the time grid and represent them as vectors in $\R^D$.
We then create a dataset $\{X, Y\}$ of $N$ elements. Each row of the dataset is of the form:
\begin{align}
	X &= [q, s_1, s_2] \\
	Y &= [\boldsymbol{A}^T_{\ell m}, \boldsymbol{\phi}^T_{\ell m}, \hdots ] 
\end{align}
%
The dataset $Y$ gathers the amplitude and phase for the different $L$ modes in the dataset.
In what follows we will refer to any of the vectors $\boldsymbol{A}_{\ell m}$ or $\boldsymbol{\phi}_{\ell m}$ as $\boldsymbol{f}$.
Note that we use the same grid for all the modes.

\subsection{Dimensionality reduction}
\label{sec:PCA}

It is unfeasibile to perform a regression targeting a large dimensional vector such as $\boldsymbol{f} \in \R^D$. For this reason, in \cite{Schmidt:2020yuu} we introduced a Principal Component Analysis (PCA) dimensionality reduction scheme.
It is an approximately invertible linear mapping between a vector $\boldsymbol{f} \in \R^D$ in a large dimensional space to lower dimensional vector  $\boldsymbol{g} \in \R^K$:
%
\begin{align}
	\mathbf{g} = H (\mathbf{f} - \boldsymbol{\mu}) \label{eq:PCA_reduction_model}\\
	\mathbf{f} = H^T \mathbf{g} + \boldsymbol{\mu} \label{eq:PCA_reconstruction_model}
\end{align}
where $\boldsymbol{\mu} \in \R^D$ and $H$ is a $K \times D$ matrix. The rows $H_{i:}$ of $H$, also called {\it Principal Components} (PC), form an orthonormal set of vector, i.e. it holds ${\sum_{k=1}^D H_{ik} H_{jk} = \delta_{ij}}$.
The PCs are the first K eigenvectors of the $D \times D$ covariance matrix of the dataset, as described in \cite[Sec. 12]{murphy2012machine}.

One can have a deeper insight on PCA considering the following formula for the reconstructed vector $\mathbf{f}$ (setting $\boldsymbol{\mu}=0$ without loss of generality):
\begin{equation} \label{eq:perturbative_exp}
	\mathbf{f} = \sum_{i=1}^K \langle \mathbf{f} | H_{i:} \rangle \; H_{i:}
\end{equation}
%
where $\langle \cdot | \cdot \rangle$ is the usual scalar product.

Since less important PCs are more orthogonal to data, the typical magnitude of $g_i = \langle \mathbf{f} | H_{i:} \rangle$ decreases as $i$ increases.\footnote{For this reason PCA can be seen as a perturbative expansion on the basis vectors $H_{i:}$, where the accuracy is roughly measured by the eigenvalues of the first neglected PC. Increasing the number K of PCs considered increases the accuracy of the model (but also the complexity of the model).}.
As a consequence, the regression for a lower order PC needs to be more accurate than the one for the higher order PC. This will be taken care by a suitable choice for the loss function for the regression (see next section).


\subsection{Neural network regression}
\label{sec:NN}

An Artificial Neural Network (ANN) is a popular regression model, consisting in a powerful parametric function, whose parameters (or weights), when properly set, can represent a large variety of relations between inputs and output.
An ANN is built by stacking together $N_\text{L}$ layers in such a way the the input of one is the output of the following. Each layer is a function $L: \R^D \rightarrow \R^{D^\prime} $ and has the following functional form

\begin{equation}
	\boldsymbol{y} = a(W\boldsymbol{x})
\end{equation}
%
Where W is $D^\prime \times D$ matrix and $a$ is an activation function that acts elementwise on the a vector.
Each component $y_i$ of the output of the layer is called node and the number of nodes is a tunable parameter, controlling the representative power of the layer.

An ANN $\mathcal{N}$ is obtained by composing $N_\text{L}$ different layers (each with a suitable number of nodes):
%
\begin{equation}
	\mathcal{N}_W = L_{N_\text{L}} \circ \hdots \circ L_2 \circ L_1
\end{equation}
%
where we denote by $W$ the set of all the parameters the ANN depends on.

The number of layers, together with the number of nodes per layer are hyperparameters that needs to be carefully chosen, to balance model accuracy and model complexity.
Another sensibile choice is the activation function: several possible choices are possible, the most popular being the {\it sigmoid}, the hyperbolic tangent or the so called ReLU function. In our work, we consider ???? function.

Once the ANN is set up, we need to set its weights to the values that achieve our regression task.
This procedure is called training, where we minimise a loss function with respect to the weights $\boldsymbol{W}$ of the model.
The loss function depends on the dataset at hand ${\{\boldsymbol{x}_i, \boldsymbol{y}_i\}_i}$.
Mathematically, the weights are given by:
%
\begin{equation}\label{eq:loss_general}
	\boldsymbol{W} = \argmin_W \mathcal{L}(W; \{\boldsymbol{x}_i, \boldsymbol{y}_i\}_i)
\end{equation}

The minimisation of the loss function is performed by stochastic gradient descent (SGD), which relies on the gradients $\partial_W \mathcal{L}$ of the loss function. The gradients are computing through the backpropagation algorithm \cite{}.

To perform our regression $\theta \longmapsto \boldsymbol{g}$, we employ an ensemble of networks, that suitably combined delivers accurate results.
To improve the representative power or the ANN, we employ feature augmentation on the vector $\boldsymbol{\vartheta} = (q, s_1, s_2)$, effectively using the augmented vector $\tilde{\boldsymbol{\vartheta}}$ as input for the regression. The features added needs to be chosen with a validation process: this will be discussed in the next section.

Before the training, the regression targets $\boldsymbol{y}_i$ are rescaled such that $\boldsymbol{y}_i \rightarrow \frac{\boldsymbol{y}_i}{\boldsymbol{w}}$, where $\boldsymbol{w}$ keeps the maximum of $|\boldsymbol{y}_i|$ along each axis. %${w}_j = \max_i(|(\boldsymbol{y}_i)_j|)$.
In this way all the regression targets span the same order or magnitude, making the regression task easier.

For the amplitude $\boldsymbol{A}_{\ell m}$ of each mode, we employ a single ANN $\mathcal{N}_{A_{\ell m}}$.
The predicted amplitude $\hat{\boldsymbol{A}}_{\ell m}$, including the PCA reconstruction, has the following form:
\begin{equation}
	\hat{\boldsymbol{A}}_{\ell m}(\boldsymbol{\vartheta}) = H_{A_{\ell m}}^T \mathcal{N}_{A_{\ell m}}(\tilde{\boldsymbol{\vartheta}}) + \boldsymbol{\mu}_{A_{\ell m}}
\end{equation}

For the phase $\boldsymbol{\phi}_{\ell m}$, we employ one ANN $\mathcal{N}_{\phi_{\ell m}\text{- 12}}$ to predict only the first two PCA components.
Another ANN will take care of the remaining components $\mathcal{N}_{\phi_{\ell m}\text{- 2:K}}$.
On top of this, we build an additional ANN $\mathcal{N}_{\phi_{\ell m}\text{- residual}}$ to target the residual of the predictions of $\mathcal{N}_{\phi_{\ell m}\text{- 12}}$.
The scheme make sure that the first two PCs are predicted with much larger accuracy than the others. Indeed, the reconstructed WF depends largely on the first two components and a small fractional error can potentially have a large impact on the overall accuracy.

The predicted phase $\hat{\boldsymbol{\phi}}_{\ell m}$ is then given by:
%
\begin{equation}
	\hat{\boldsymbol{\phi}}_{\ell m}(\boldsymbol{\vartheta}) = \boldsymbol{\mu}_{\phi_{\ell m}} + H_{\phi_{\ell m}}^T 
	\begin{pmatrix}
        \mathcal{N}_{\phi_{\ell m}\text{- 12}}(\tilde{\boldsymbol{\vartheta}}) + \mathcal{N}_{\phi_{\ell m}\text{- residual}}(\tilde{\boldsymbol{\vartheta}}) \\
        \mathcal{N}_{\phi_{\ell m}\text{- 2:K}}(\tilde{\boldsymbol{\vartheta}}) \hfill
	 \end{pmatrix}.
\end{equation}


We train our model using the PCA dataset, obtained by PCA reducing the training set. Each ANN is trained using the following loss function:
\begin{equation}
	\mathcal{L} = \frac{1}{N} \sum_{i=1}^N \Bigl((\mathcal{N}(\boldsymbol{\vartheta}_i) - \boldsymbol{y}_i)}\boldsymbol{w} \Bigr)^2
\end{equation}
%
where $\boldsymbol{y}_i$ is the (scaled) regression target of each network and $\boldsymbol{w} \in \R^K$ takes into account the fact that different PCs have different orders of magnitude.

%%%%%%%%%%%%%%%%%%%
\section{Performance study}
\label{sec:performance}
In this section, we first study how the model performance depends on the different choices of hyperparameters (network architecture, learning rate, features etc...): this makes sure that we reproduce our data at best.
We then evaluate the accuracy of our model and report the speed up that we obtain when using \texttt{mlgw-NN} instead of the training model \texttt{SEOBNRv4PHM}.

In what follows, we use the {\it match} $\mathcal{M}$ to measure the discrepancy between two waveforms.
For two modes $h^1_{\ell m}$, $h^2_{\ell m}$ we define the match as:
\begin{equation}
	\mathcal{M}	= \max_t \int_{-\infty}^{\infty} \text{d}f \; \frac{{\tilde{\hat{h}}^*}^1_{\ell m} \tilde{\hat{h}}^2_{\ell m}}{S_n(f)} e^{i2\pi ft}
\end{equation}
where $\hat{h}$ refers to a normalized waveform (i.e. with an unitary match with itself) and $\tilde{\phantom{h}}$ denotes the Fourier transform.

Scalar product

Match for modes

Symphony match

\subsection{Hyperparameter optimization}
\label{sec:hyperparameter}

\subsection{Accuracy study}

\begin{table*}
\begin{tabular}{lllll}
\toprule
mode &                                       amp &                    ph &                                   ph\_2345 &           ph\_residual \\
\midrule
22 &  [9.47e-06, 2.12e-04, 1.86e-03, 6.74e-03] &  [3.60e-08, 8.25e-07] &  [3.97e-05, 1.08e-04, 6.22e-04, 2.11e-03] &  [2.87e-02, 9.43e-03] \\
21 &  [4.11e-01, 3.85e-01, 1.45e-01, 7.55e-02] &  [1.50e-05, 3.13e-05] &  [2.09e-03, 3.07e-02, 3.70e-02, 5.01e-02] &  [1.01e-02, 6.41e-03] \\
33 &  [2.68e-02, 2.59e-02, 1.17e-01, 8.74e-02] &  [1.23e-05, 1.73e-05] &  [1.29e-04, 6.78e-04, 9.13e-02, 6.50e-02] &  [1.92e-02, 3.01e-03] \\
44 &  [5.63e-01, 3.27e-01, 1.36e-01, 1.42e-01] &  [1.68e-05, 1.89e-06] &  [1.65e-04, 1.29e-03, 1.47e-01, 2.05e-01] &  [3.36e-02, 1.89e-02] \\
55 &  [2.19e-02, 6.88e-02, 1.78e-01, 5.95e-02] &  [3.49e-05, 2.02e-06] &  [1.21e-04, 2.53e-01, 2.07e-02, 2.77e-02] &  [2.06e-02, 2.28e-02] \\
\bottomrule
\end{tabular}
\caption{Table for the model accuracy}
\label{tab:table_accuracy}
\end{table*}

\label{sec:accuracy}
\begin{itemize}
	\item mode by mode
	\item overall
\end{itemize}

\subsection{Parameter Estimation}
\label{sec:PE}
gw150914? Or other events?

\subsection{Timing study}
\label{sec:timing}

%%%%%%%%%%%%%%%%%%%

\section{Final remarks and future prospects}
\label{sec:end}
\blindtext


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END & BIB


        \begin{acknowledgments}
          %
          This research has made use of data, software and/or web tools obtained 
          from the Gravitational Wave Open Science Center (https://www.gw-openscience.org), 
          a service of LIGO Laboratory, the LIGO Scientific Collaboration and the 
          Virgo Collaboration. LIGO is funded by the U.S. National Science Foundation. 
          Virgo is funded by the French Centre National de Recherche Scientifique (CNRS), 
          the Italian Istituto Nazionale della Fisica Nucleare (INFN) and the 
          Dutch Nikhef, with contributions by Polish and Hungarian institutes.
        \end{acknowledgments}

	\bibliography{biblio.bib}
	\bibliographystyle{ieeetr}

\end{document}



